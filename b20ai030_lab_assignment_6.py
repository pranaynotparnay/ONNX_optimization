# -*- coding: utf-8 -*-
"""B20AI030_Lab_Assignment_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pbr1oVdqWJbyg15A6c2yfOBr4Cgqypkt
"""

!pip install torch-tb-profiler

!pip install onnx
!pip install onnxruntime

!pip install onnxoptimizer

!pip install timm



import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim
import torch.profiler
import torch.utils.data
import torchvision.datasets
import torchvision.models
import torchvision.transforms as T
import torch.nn.functional as F
import torchvision.models as models
import torch.quantization
from torch.autograd import Variable
from torch.utils.tensorboard import SummaryWriter
import torch.onnx

import numpy as np
from time import perf_counter
from copy import deepcopy
import io

import onnxruntime
import time
import os

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762])
])


transform_test = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



def train(model, dataloader, epoch):
    model.train()
    train_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (inputs, targets) in enumerate(dataloader):
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = loss_function(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    # saving the training loss and accuracy to Tensorboard
    writer.add_scalar('Training Loss', train_loss / len(dataloader.dataset), epoch)
    writer.add_scalar('Training Accuracy', 100. * correct / total, epoch)

    print('Epoch: %d Train Loss: %.3f Train Acc: %.3f' % (epoch, train_loss / len(dataloader.dataset), 100. * correct / total))



"""# **Resnet34**"""

from torchvision.models import resnet34 , ResNet34_Weights

model = resnet34(weights=ResNet34_Weights.DEFAULT)
model.fc = nn.Linear(512, 100)

model.to(device)

loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

#initializing a Tensorboard summarywriter to log the training metrics
writer = SummaryWriter()



"""Train the model for a specified number of epochs, profiling the training procedure using TensorBoard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/resnet34'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
  for epoch in range(1,3):
    train(model,trainloader,epoch)
    prof.step()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
#%tensorboard --logdir log
# %tensorboard --logdir /content/logs/resnet34

# #  %reload_ext tensorboard



"""The model size and average execution time before performing the
the mentioned inferencing techniques on the test dataset
"""

model_size = sum(p.numel() * p.element_size() for p in model.parameters())
print(f"Model size: {model_size / 1024 / 1024:.6f} MB")

# Run the model on the test dataset and measure the execution time
start_time = time.time()
for inputs, targets in testloader:
    outputs = model(inputs.to(device))
    loss = loss_function(outputs, targets.to(device))
avg_execution_time = (time.time() - start_time) / len(testloader.dataset)

print(f"Average execution time: {avg_execution_time:.6f} seconds")



model.eval()



"""# **ONNX ; ONNX Quantized Inferencing**"""

for data,label in testloader:
  dummy_input = data
  break

model.cpu()

input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(model,
                 dummy_input,
                 "resnet34.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

ort_session = onnxruntime.InferenceSession("resnet34.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = model(dummy_input) #torch.randn(1, 3, 224, 224)
np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

!python -m onnxoptimizer resnet34.onnx resnet34_opt.onnx

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('resnet34.onnx')))
print('Average runtime of ONNX Optimized Model in GPU: ' + str(time_ort_model_evaluation('resnet34_opt.onnx')))

def quantize_onnx_model(onnx_model_path, quantized_model_path):
    from onnxruntime.quantization import quantize_dynamic, QuantType
    import onnx
    onnx_opt_model = onnx.load(onnx_model_path)
    quantize_dynamic(onnx_model_path,
                     quantized_model_path,
                     weight_type=QuantType.QUInt8) #QInt8

    print(f"quantized model saved to:{quantized_model_path}")

quantize_onnx_model('resnet34_opt.onnx', 'resnet34_opt_quant.onnx')

print('ONNX full precision model size (MB):', os.path.getsize("resnet34_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("resnet34_opt_quant.onnx")/(1024*1024))

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in TPU: ' + str(time_ort_model_evaluation('resnet34.onnx')))
print('Average runtime of ONNX Quantized Model in TPU: ' + str(time_ort_model_evaluation('resnet34_opt_quant.onnx')))





"""# **Torchscript Inferrrencing**"""

# Converting the model to Torchccript
scripted_model = torch.jit.script(model)
torch.jit.save(scripted_model, "resnet34_scripted.pt")

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

"""#Runtime performance of TorchScript Model in CPU"""

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("resnet34_scripted.pt")

print(f"TorchScript Model size ==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")



"""# Runtime performance of TorchScript Model in GPU"""

scripted_model.cuda()

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images).to(device)
        labels=labels.to(device)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("resnet34_scripted.pt")

print(f"TorchScript Model size==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")



"""# **DenseNet-121**"""

model = models.densenet121(pretrained=True)

model.fc = nn.Linear(512, 100)

model.to(device)

loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

#initializing a Tensorboard summarywriter to log the training metrics
writer = SummaryWriter()



"""Train the model for a specified number of epochs, profiling the training procedure using TensorBoard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/densenet121'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
  for epoch in range(1,3):
    train(model,trainloader,epoch)
    prof.step()



# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
#%tensorboard --logdir log
# %tensorboard --logdir /content/logs/densenet121

# #  %reload_ext tensorboard





"""The model size and average execution time before performing the
the mentioned inferencing techniques on the test dataset
"""

model_size = sum(p.numel() * p.element_size() for p in model.parameters())
print(f"Model size: {model_size / 1024 / 1024:.6f} MB")

# Run the model on the test dataset and measure the execution time
start_time = time.time()
for inputs, targets in testloader:
    outputs = model(inputs.to(device))
    loss = loss_function(outputs, targets.to(device))
avg_execution_time = (time.time() - start_time) / len(testloader.dataset)

print(f"Average execution time: {avg_execution_time:.6f} seconds")



model.eval()



"""# **ONNX ; ONNX Quantized Inferencing**"""

for data,label in testloader:
  dummy_input = data
  break

model.cpu()

input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(model,
                 dummy_input,
                 "densenet121.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

ort_session = onnxruntime.InferenceSession("densenet121.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = model(dummy_input)
np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

!python -m onnxoptimizer densenet121.onnx densenet121_opt.onnx

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('densenet121.onnx')))
print('Average runtime of ONNX Optimized Model in GPU: ' + str(time_ort_model_evaluation('densenet121_opt.onnx')))

def quantize_onnx_model(onnx_model_path, quantized_model_path):
    from onnxruntime.quantization import quantize_dynamic, QuantType
    import onnx
    onnx_opt_model = onnx.load(onnx_model_path)
    quantize_dynamic(onnx_model_path,
                     quantized_model_path,
                     weight_type=QuantType.QUInt8) #QInt8

    print(f"quantized model saved to:{quantized_model_path}")

quantize_onnx_model('densenet121_opt.onnx', 'densenet121_opt_quant.onnx')

print('ONNX full precision model size (MB):', os.path.getsize("densenet121_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("densenet121_opt_quant.onnx")/(1024*1024))

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in TPU: ' + str(time_ort_model_evaluation('densenet121.onnx')))
print('Average runtime of ONNX Quantized Model in TPU: ' + str(time_ort_model_evaluation('densenet121_opt_quant.onnx')))





"""# **Torchscript Inferencing**"""

# Converting the model to Torchccript
scripted_model = torch.jit.script(model)
torch.jit.save(scripted_model, "densenet121_scripted.pt")

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

"""#Runtime performance of TorchScript Model in CPU"""

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("densenet121_scripted.pt")

print(f"TorchScript Model size ==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")



"""# Runtime performance of TorchScript Model in GPU"""

scripted_model.cuda()

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images).to(device)
        labels=labels.to(device)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("densenet121_scripted.pt")

print(f"TorchScript Model size==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")





"""# **EfficientNet-B0**"""

import timm

# Load the pre-trained EfficientNet-B0 model
model = timm.create_model('efficientnet_b0', pretrained=True)

model.fc = nn.Linear(512, 100)

model.to(device)

loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

#initializing a Tensorboard summarywriter to log the training metrics
writer = SummaryWriter()



"""Train the model for a specified number of epochs, profiling the training procedure using TensorBoard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/efficientnet_b0'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
  for epoch in range(1,3):
    train(model,trainloader,epoch)
    prof.step()



# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
#%tensorboard --logdir log
# %tensorboard --logdir /content/logs/efficientnet_b0

#  %reload_ext tensorboard



"""The model size and average execution time before performing the
the mentioned inferencing techniques on the test dataset
"""

model_size = sum(p.numel() * p.element_size() for p in model.parameters())
print(f"Model size: {model_size / 1024 / 1024:.6f} MB")

# Run the model on the test dataset and measure the execution time
start_time = time.time()
for inputs, targets in testloader:
    outputs = model(inputs.to(device))
    loss = loss_function(outputs, targets.to(device))
avg_execution_time = (time.time() - start_time) / len(testloader.dataset)

print(f"Average execution time: {avg_execution_time:.6f} seconds")



model.eval()



"""# **ONNX ; ONNX Quantized Inferencing**"""

for data,label in testloader:
  dummy_input = data
  break

model.cpu()

input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(model,
                 dummy_input,
                 "efficientnet_b0.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

ort_session = onnxruntime.InferenceSession("efficientnet_b0.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = model(dummy_input)
np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

!python -m onnxoptimizer efficientnet_b0.onnx efficientnet_b0_opt.onnx

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('efficientnet_b0.onnx')))
print('Average runtime of ONNX Optimized Model in GPU: ' + str(time_ort_model_evaluation('efficientnet_b0_opt.onnx')))

def quantize_onnx_model(onnx_model_path, quantized_model_path):
    from onnxruntime.quantization import quantize_dynamic, QuantType
    import onnx
    onnx_opt_model = onnx.load(onnx_model_path)
    quantize_dynamic(onnx_model_path,
                     quantized_model_path,
                     weight_type=QuantType.QUInt8) #QInt8

    print(f"quantized model saved to:{quantized_model_path}")

quantize_onnx_model('efficientnet_b0_opt.onnx', 'efficientnet_b0_opt_quant.onnx')

print('ONNX full precision model size (MB):', os.path.getsize("efficientnet_b0_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("efficientnet_b0_opt_quant.onnx")/(1024*1024))

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in TPU: ' + str(time_ort_model_evaluation('efficientnet_b0.onnx')))
print('Average runtime of ONNX Quantized Model in TPU: ' + str(time_ort_model_evaluation('efficientnet_b0_opt_quant.onnx')))





"""# **TorchscripT inferencing**"""

# Converting the model to Torchccript
scripted_model = torch.jit.script(model)
torch.jit.save(scripted_model, "efficientnet_b0_scripted.pt")

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

"""#Runtime performance of TorchScript Model in CPU"""

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("efficientnet_b0_scripted.pt")

print(f"TorchScript Model size ==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")



"""# Runtime performance of TorchScript Model in GPU"""

scripted_model.cuda()

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images).to(device)
        labels=labels.to(device)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("efficientnet_b0_scripted.pt")

print(f"TorchScript Model size==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")







"""# **ConvNeXt-T**"""

model = models.convnext_base(pretrained=True)

model.fc = nn.Linear(512, 100)

model.to(device)

loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

#initializing a Tensorboard summarywriter to log the training metrics
writer = SummaryWriter()



"""Train the model for a specified number of epochs, profiling the training procedure using TensorBoard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/convnext_base'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
  for epoch in range(1,3):
    train(model,trainloader,epoch)
    prof.step()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir log
# %tensorboard --logdir /content/logs/convnext_base

# #  %reload_ext tensorboard



"""The model size and average execution time before performing the
the mentioned inferencing techniques on the test dataset
"""

model_size = sum(p.numel() * p.element_size() for p in model.parameters())
print(f"Model size: {model_size / 1024 / 1024:.6f} MB")

# Run the model on the test dataset and measure the execution time
start_time = time.time()
for inputs, targets in testloader:
    outputs = model(inputs.to(device))
    loss = loss_function(outputs, targets.to(device))
avg_execution_time = (time.time() - start_time) / len(testloader.dataset)

print(f"Average execution time: {avg_execution_time:.6f} seconds")



model.eval()



"""# **ONNX ; ONNX Quantized Inferencing**"""

for data,label in testloader:
  dummy_input = data
  break

model.cpu()

input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(model,
                 dummy_input,
                 "convnext_base.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

ort_session = onnxruntime.InferenceSession("convnext_base.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = model(dummy_input)
np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

!python -m onnxoptimizer convnext_base.onnx convnext_base_opt.onnx

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('convnext_base.onnx')))
print('Average runtime of ONNX Optimized Model in GPU: ' + str(time_ort_model_evaluation('convnext_base_opt.onnx')))

def quantize_onnx_model(onnx_model_path, quantized_model_path):
    from onnxruntime.quantization import quantize_dynamic, QuantType
    import onnx
    onnx_opt_model = onnx.load(onnx_model_path)
    quantize_dynamic(onnx_model_path,
                     quantized_model_path,
                     weight_type=QuantType.QUInt8) #QInt8

    print(f"quantized model saved to:{quantized_model_path}")

quantize_onnx_model('convnext_base_opt.onnx', 'convnext_base_opt_quant.onnx')

print('ONNX full precision model size (MB):', os.path.getsize("convnext_base_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("convnext_base_opt_quant.onnx")/(1024*1024))

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model : ' + str(time_ort_model_evaluation('convnext_base.onnx')))
print('Average runtime of ONNX Quantized Model: ' + str(time_ort_model_evaluation('convnext_base_opt_quant.onnx')))





"""# **Torchscript Inferencing**"""

# Converting the model to Torchccript
scripted_model = torch.jit.script(model)
torch.jit.save(scripted_model, "convnext_base_scripted.pt")

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

"""#Runtime performance of TorchScript Model in CPU"""

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("convnext_base_scripted.pt")

print(f"TorchScript Model size ==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")





"""# Runtime performance of TorchScript Model in GPU"""

scripted_model.cuda()

#evaluating the Torchccript model on the test datset
def evaluate_model(model, test_loader):
    total_time = 0
    correct = 0
    total = 0

    for images, labels in testloader:
        images = Variable(images).to(device)
        labels=labels.to(device)
        start_time = time.time()
        outputs = model(images)
        total_time += time.time() - start_time
        predicted = outputs.argmax(dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_time = total_time / total

    return accuracy, average_time

scripted_accuracy, scripted_average_time = evaluate_model(scripted_model, testloader)

scripted_model_size = os.path.getsize("convnext_base_scripted.pt")

print(f"TorchScript Model size==> {scripted_model_size / (1024**2):.2f} MB")
print(f"TorchScript Model accuracy==> {scripted_accuracy:.2f}%")
print(f"TorchScript Model average execution time==> {scripted_average_time * 1000:.2f} ms")





